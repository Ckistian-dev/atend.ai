from google import genai
from google.genai import types
import logging
import json
import asyncio
import pytz
import re
from typing import Optional, List, Dict, Any
from collections.abc import Set
import numpy as np
from datetime import datetime, timezone

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select


from app.core.config import settings
from app.db import models
from app.crud import crud_user, crud_atendimento

logger = logging.getLogger(__name__)

class SetEncoder(json.JSONEncoder):
    """Codificador JSON para lidar com objetos 'set'."""
    def default(self, obj):
        if isinstance(obj, Set):
            return list(obj)
        return super().default(obj)

class GeminiService:
    def __init__(self):
        keys_str = settings.GOOGLE_API_KEYS
        self.api_keys = [key.strip() for key in keys_str.split(',') if key.strip()]

        if not self.api_keys:
            logger.error("üö® ERRO CR√çTICO: Nenhuma chave de API do Google foi configurada em GOOGLE_API_KEYS.")
            raise ValueError("A lista de GOOGLE_API_KEYS n√£o pode estar vazia.")
            
        self.current_key_index = 0
        self.generation_config = {
            "temperature": 0.5,        # Aumentei: Deixa a fala menos "dura" e mais coloquial.
            "top_p": 0.95,             # Ajuste fino: Mant√©m a coer√™ncia mas corta alucina√ß√µes absurdas.
            "top_k": 40,               # O PULO DO GATO: De 1 para 40. Permite variar o vocabul√°rio.
            "frequency_penalty": 0.6,  # CR√çTICO: Penaliza palavras que ele j√° falou muito (evita o "√ìtimo!" repetido).
            "presence_penalty": 0.4    # Ajuda a n√£o ficar repetindo o que o usu√°rio acabou de dizer.
        }
        # NOVO: Multiplicador para o custo de tokens de output, para normalizar pelo custo de input.
        # Baseado no custo informado: Input $0,30, Output $2,50 por milh√£o de tokens.
        self.output_token_multiplier = 2.5 / 0.3
        
        self._initialize_model()

    def _initialize_model(self):
        """Inicializa o cliente Gemini com a chave atual usando o novo SDK."""
        try:
            current_key = self.api_keys[self.current_key_index]
            
            # NOVO SDK: Instancia o Client
            # http_options={'api_version': 'v1alpha'} pode ser usado se precisar de recursos beta
            self.client = genai.Client(api_key=current_key)
            
            logger.info(f"‚úÖ Cliente Gemini (New SDK) inicializado com sucesso (chave √≠ndice {self.current_key_index}).")
        except Exception as e:
            logger.error(f"üö® ERRO CR√çTICO ao configurar o Gemini com a chave √≠ndice {self.current_key_index}: {e}", exc_info=True)
            raise

    def _rotate_key(self):
        """Muda para a pr√≥xima chave na lista."""
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        logger.warning(f"Alternando para a chave de API do Google com √≠ndice {self.current_key_index}.")
        self._initialize_model()
        return self.current_key_index

    def _parse_json_response(self, response_text: str) -> dict:
        """Limpa e parseia JSON da resposta da IA, com tratamento de erros de escape."""
        clean_response = response_text.strip().replace("```json", "").replace("```", "")
        
        try:
            return json.loads(clean_response)
        except json.JSONDecodeError:
            # Tenta corrigir backslashes soltos (comum em caminhos de arquivo ou LaTeX)
            # Regex: Backslash n√£o seguido de escape v√°lido -> Dobra o backslash
            try:
                fixed_response = re.sub(r'\\(?![/\"\\bfnrtu])', r'\\\\', clean_response)
                return json.loads(fixed_response)
            except json.JSONDecodeError:
                raise

    async def _generate_with_retry(
        self, 
        prompt: Any, 
        db: AsyncSession, 
        user: models.User, 
        is_media: bool = False,
        system_instruction: Optional[str] = None,
        atendimento_id: Optional[int] = None
    ):  # Removido o tipo de retorno estrito para evitar erros de importa√ß√£o cruzada por enquanto
        """
        Executa a chamada para a API Gemini (Novo SDK), deduz token e rotaciona chaves.
        """
        
        # Configura√ß√£o do novo SDK
        # Adaptamos o dicion√°rio antigo para o novo objeto de configura√ß√£o
        config_args = {
            "temperature": self.generation_config.get("temperature", 0.5),
            "top_p": self.generation_config.get("top_p", 1),
            "top_k": self.generation_config.get("top_k", 1),
        }

        if not is_media and isinstance(prompt, str):
            logger.debug(f"Prompt (texto) para a IA: {prompt[:500]}...")
            # No novo SDK, response_mime_type entra na config
            config_args["response_mime_type"] = "application/json"

        # Adiciona system_instruction se fornecido
        if system_instruction:
            config_args["system_instruction"] = system_instruction

        # Cria o objeto de configura√ß√£o tipado
        gen_config = types.GenerateContentConfig(**config_args)

        # --- DEBUG: PRINT PROMPT ---
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            debug_msg = f"\n{'='*20} PROMPT ENVIADO PARA IA [{timestamp}] {'='*20}\n"
            
            if system_instruction:
                debug_msg += f"--- SYSTEM INSTRUCTION ---\n{system_instruction}\n{'-'*30}\n"

            if isinstance(prompt, str):
                debug_msg += f"{prompt}\n"
            elif isinstance(prompt, list):
                for p in prompt:
                    if isinstance(p, str):
                        debug_msg += f"[TEXTO]: {p}\n"
                    else:
                        debug_msg += f"[M√çDIA/OBJETO]: {type(p)}\n"
            debug_msg += f"{'='*60}\n"
            
            # Salva o prompt em arquivo (append/log)
            with open("last_prompt.txt", "a", encoding="utf-8") as f:
                f.write(debug_msg)
        except Exception as e:
            print(f"Erro ao printar/salvar prompt: {e}")

        initial_key_index = self.current_key_index
        max_attempts_per_key = 2
        
        while True:
            for attempt in range(max_attempts_per_key):
                try:
                    logger.info(
                        f"Tentando gerar conte√∫do com a chave √≠ndice {self.current_key_index} "
                        f"(tentativa {attempt + 1}/{max_attempts_per_key})."
                    )
                    
                    # --- MUDAN√áA PRINCIPAL: Chamada Ass√≠ncrona Nativa (.aio) ---
                    # N√£o precisa mais de run_in_executor
                    response = await self.client.aio.models.generate_content(
                        model='gemini-2.5-flash', # Modelo corrigido para vers√£o est√°vel e mais recente
                        contents=prompt,
                        config=gen_config
                    )
                    
                    # --- L√ìGICA DE TOKEN (OD√îMETRO) ---
                    # Extrai o uso real de tokens da resposta do Gemini
                    usage_metadata = response.usage_metadata
                    tokens_to_deduct = 0 # Inicializa com 0

                    if usage_metadata:
                        input_tokens = usage_metadata.prompt_token_count
                        output_tokens = usage_metadata.candidates_token_count
                        
                        # Calcula o custo equivalente em "tokens de input"
                        equivalent_total_tokens = input_tokens + (output_tokens * self.output_token_multiplier)
                        
                        # Arredonda para o inteiro mais pr√≥ximo para dedu√ß√£o
                        tokens_to_deduct = round(equivalent_total_tokens)

                        logger.info(
                            f"Uso de tokens (User {user.id}): "
                            f"Input={input_tokens}, Output={output_tokens}. "
                            f"Custo Equivalente (x{self.output_token_multiplier:.2f}) = {tokens_to_deduct} tokens."
                        )
                    else:
                        logger.warning(f"N√£o foi poss√≠vel obter metadados de uso de tokens para o user {user.id}.")

                    try:
                        if tokens_to_deduct > 0:
                            logger.info(f"Sucesso na chamada √† API Gemini para o utilizador {user.id}. Deduzindo {tokens_to_deduct} tokens.")
                            await crud_user.decrement_user_tokens(db, db_user=user, usage=tokens_to_deduct, atendimento_id=atendimento_id)
                            await db.commit()
                            await db.refresh(user)
                    except Exception as token_err:
                        logger.error(f"Falha ao deduzir tokens: {token_err}", exc_info=True)
                        await db.rollback()
                    
                    return response

                # Captura erros do novo SDK (geralmente ServerError ou ClientError)
                # O erro 429 (Quota) agora geralmente vem como um ClientError com status 429
                except Exception as e:
                    error_str = str(e).lower()
                    
                    # Detec√ß√£o de Erro de Cota (429) ou Recurso Esgotado
                    if "429" in error_str or "resource exhausted" in error_str or "quota" in error_str:
                        logger.warning(f"Quota da API excedida (429) com a chave {self.current_key_index}. Rotacionando...")
                        break # Sai do loop 'for' para rotacionar a chave
                    
                    # Detec√ß√£o de bloqueio de seguran√ßa ou prompt inv√°lido
                    elif "blocked" in error_str or "invalid argument" in error_str:
                        logger.error(f"Erro n√£o recuper√°vel (Bloqueio/Inv√°lido): {e}")
                        raise e
                        
                    else:
                        # Erros gen√©ricos de conex√£o/servidor
                        logger.error(f"Erro inesperado na API Gemini: {e}. Tentativa {attempt + 1}.")
                        await asyncio.sleep(2) # Espera um pouco antes de tentar de novo na mesma chave
            
            # Se saiu do loop 'for', significa que precisa trocar de chave
            new_key_index = self._rotate_key()
            
            if new_key_index == initial_key_index:
                logger.critical(f"Todas as {len(self.api_keys)} chaves de API falharam.")
                raise Exception("Todas as chaves de API excederam a quota.")

    async def transcribe_and_analyze_media(
        self, 
        media_data: dict,  # Espera receber: {"data": bytes, "mime_type": str}
        db_history: List[dict], 
        persona: models.Config,
        db: AsyncSession,
        user: models.User,
        atendimento_id: Optional[int] = None
    ) -> str:
        logger.info(f"Iniciando transcri√ß√£o/an√°lise para m√≠dia do tipo {media_data.get('mime_type')}")
        
        # --- 1. PREPARA√á√ÉO DA M√çDIA PARA O NOVO SDK ---
        try:
            file_bytes = media_data.get("data")
            mime_type = media_data.get("mime_type")

            if not file_bytes:
                raise ValueError("Bytes do arquivo n√£o encontrados em media_data")

            # Cria o objeto Part nativo do novo SDK
            # Isso substitui a l√≥gica antiga de upload ou passagem de objetos complexos
            media_part = types.Part.from_bytes(
                data=file_bytes, 
                mime_type=mime_type
            )
        except Exception as e:
            logger.error(f"Erro ao preparar objeto de m√≠dia para o Gemini: {e}")
            return "[Erro interno ao processar o arquivo de m√≠dia]"

        # --- 2. MONTAGEM DO PROMPT (Lista de conte√∫dos) ---
        prompt_contents = []
        system_instruction = None
        
        # L√≥gica para √Åudio (Transcri√ß√£o)
        if 'audio' in mime_type or 'mpeg' in mime_type or 'ogg' in mime_type:
            task_text = "Sua √∫nica tarefa √© transcrever o √°udio a seguir. Retorne apenas o texto transcrito, sem adicionar nenhuma outra palavra, introdu√ß√£o ou formata√ß√£o."
            prompt_contents = [task_text, media_part]
            
        # L√≥gica para Imagem/Documento (An√°lise Visual)
        else:
            system_instruction = "Voc√™ √© um especialista em an√°lise visual e interpreta√ß√£o de conte√∫do."
            
            history_str = self._format_history_optimized(db_history)
            
            prompt_text = (
                f"## HIST√ìRICO RECENTE\n{history_str}\n\n"
                "## INSTRU√á√ÉO DE AN√ÅLISE\n"
                "Analise a m√≠dia fornecida (imagem, documento ou v√≠deo) e descreva seu conte√∫do de forma abrangente e detalhada. O objetivo √© fornecer um resumo completo que capture todos os aspectos importantes para que a IA de conversa√ß√£o possa entender o contexto sem precisar 'ver' a m√≠dia.\n\n"
                "1. **Descri√ß√£o Geral:** Comece com uma descri√ß√£o geral do que a m√≠dia mostra (ex: 'Foto de uma piscina em um jardim', 'Documento de or√ßamento', 'V√≠deo curto mostrando o funcionamento de um produto').\n"
                "2. **Detalhes Visuais (para imagens/v√≠deos):** Descreva os elementos principais, cores, ambiente, pessoas, objetos, texto vis√≠vel, e qualquer detalhe que pare√ßa relevante para a conversa.\n"
                "3. **Extra√ß√£o de Dados (se aplic√°vel):** Se a m√≠dia contiver dados estruturados (como tabelas, listas, pre√ßos, nomes, endere√ßos, datas, valores em um comprovante), extraia-os de forma clara.\n"
                "4. **Contexto e Inten√ß√£o:** Com base no hist√≥rico da conversa, tente inferir a inten√ß√£o do usu√°rio ao enviar a m√≠dia. O que ele quer mostrar ou perguntar?\n"
                "5. **Formato da Resposta:** Retorne um texto claro e bem estruturado. Use bullet points (*) para listas, se ajudar na clareza. N√£o converse, apenas forne√ßa a an√°lise."
            )
            
            # Ordem: Prompt de texto primeiro, M√≠dia depois (ou vice-versa, Gemini entende ambos)
            prompt_contents = [prompt_text, media_part]

        # --- 3. CHAMADA √Ä API ---
        try:
            # Passamos a lista (texto + m√≠dia) para o m√©todo que criamos anteriormente
            # O _generate_with_retry j√° est√° preparado para receber 'prompt' como string OU lista
            response = await self._generate_with_retry(prompt_contents, db, user, is_media=True, system_instruction=system_instruction, atendimento_id=atendimento_id)
            
            transcription = response.text.strip()
            logger.info(f"Transcri√ß√£o/An√°lise gerada: '{transcription[:100]}...'")
            return transcription
            
        except Exception as e:
            logger.error(f"Erro ao transcrever/analisar m√≠dia com Gemini: {e}", exc_info=True)
            return f"[Erro ao processar m√≠dia: {mime_type}]"

    async def generate_embedding(self, text: str) -> List[float]:
        """Gera embedding para um texto usando o modelo do Google (text-embedding-004)."""
        try:
            # O novo SDK usa client.aio.models.embed_content
            response = await self.client.aio.models.embed_content(
                model="text-embedding-004",
                contents=text
            )
            if response.embeddings:
                return response.embeddings[0].values
            return []
        except Exception as e:
            logger.error(f"Erro ao gerar embedding: {e}")
            return []

    async def generate_embeddings_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """Gera embeddings para uma lista de textos em lotes (batching)."""
        all_embeddings = []
        
        # Divide a lista total em peda√ßos menores (chunks) para respeitar limites da API
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            try:
                # O novo SDK suporta lista de strings em 'contents' para processamento em lote
                response = await self.client.aio.models.embed_content(
                    model="text-embedding-004",
                    contents=batch
                )
                
                if response.embeddings:
                    # Extrai os valores de cada embedding retornado, mantendo a ordem
                    batch_embeddings = [e.values for e in response.embeddings]
                    all_embeddings.extend(batch_embeddings)
                else:
                    logger.warning(f"Batch {i} retornou sem embeddings.")
                    all_embeddings.extend([[] for _ in batch])

            except Exception as e:
                logger.error(f"Erro ao gerar embeddings em lote (√≠ndice {i}): {e}")
                # Adiciona listas vazias para n√£o quebrar o alinhamento dos √≠ndices com os textos originais
                all_embeddings.extend([[] for _ in batch])
        
        return all_embeddings

    async def _retrieve_rag_context(self, db: AsyncSession, config_id: int, query_text: str) -> str:
        """Busca contexto relevante na base vetorial (PGVector) usando similaridade de cosseno."""
        if not query_text: return ""
        
        # 1. Gera o embedding da pergunta do usu√°rio
        query_embedding = await self.generate_embedding(query_text)
        
        if not query_embedding:
            logger.warning("Falha ao gerar embedding da query. Retornando vazio.")
            return ""

        # 2. Busca diversificada por Origem (Abas e Drive)
        # Identifica todas as origens distintas (ex: 'Pre√ßos', 'FAQ', 'drive') para este config
        stmt_origins = select(models.KnowledgeVector.origin).where(
            models.KnowledgeVector.config_id == config_id
        ).distinct()
        
        result_origins = await db.execute(stmt_origins)
        origins = result_origins.scalars().all()
        
        final_vectors = []
        
        # Para cada origem encontrada (cada aba e o drive), busca os 5 mais relevantes
        for origin in origins:
            stmt_origin = select(models.KnowledgeVector).where(
                models.KnowledgeVector.config_id == config_id,
                models.KnowledgeVector.origin == origin
            ).order_by(
                models.KnowledgeVector.embedding.cosine_distance(query_embedding)
            ).limit(5)
            
            result_origin = await db.execute(stmt_origin)
            vectors = result_origin.scalars().all()
            final_vectors.extend(vectors)
        
        if not final_vectors: return ""
        
        # Extrai conte√∫do e remove duplicatas
        chunks = [v.content for v in final_vectors]
        unique_chunks = list(dict.fromkeys(chunks))
        
        # Formata√ß√£o inteligente: Agrupa chunks com cabe√ßalhos repetidos
        formatted_text = ""
        previous_lines = []

        for chunk in unique_chunks:
            current_lines = chunk.strip().split('\n')
            
            # Verifica quantas linhas iniciais s√£o iguais ao chunk anterior
            match_count = 0
            min_len = min(len(previous_lines), len(current_lines))
            
            for i in range(min_len):
                if previous_lines[i] == current_lines[i]:
                    match_count += 1
                else:
                    break
            
            # Se houver correspond√™ncia de cabe√ßalho (pelo menos 1 linha), adiciona apenas o restante
            if match_count > 0:
                new_content = "\n".join(current_lines[match_count:])
                if new_content:
                    formatted_text += "\n" + new_content
            else:
                # Se n√£o houver correspond√™ncia, adiciona o chunk inteiro
                if formatted_text:
                    formatted_text += "\n\n"
                formatted_text += chunk.strip()
            
            previous_lines = current_lines

        return formatted_text

    def _get_datetime_context(self, user: models.User) -> str:
        """Gera o contexto de data e hora atual, incluindo dia da semana em PT-BR."""
        now_utc = datetime.now(timezone.utc)
        
        datetime_context = f"Data e Hora Atuais (UTC): {now_utc.strftime('%Y-%m-%d %H:%M:%S')} UTC\n"

        user_timezone_str = "America/Sao_Paulo" # Default
        if user.followup_config and isinstance(user.followup_config, dict):
            user_timezone_str = user.followup_config.get("timezone", "America/Sao_Paulo")
        
        try:
            user_tz = pytz.timezone(user_timezone_str)
            now_local = now_utc.astimezone(user_tz)
            
            weekdays = ["Segunda-feira", "Ter√ßa-feira", "Quarta-feira", "Quinta-feira", "Sexta-feira", "S√°bado", "Domingo"]
            weekday_name = weekdays[now_local.weekday()]
            
            datetime_context += f"Data e Hora Local do Usu√°rio ({user_timezone_str}): {now_local.strftime('%Y-%m-%d %H:%M:%S')} ({weekday_name})\n"
        except pytz.UnknownTimeZoneError:
            logger.warning(f"Timezone desconhecida '{user_timezone_str}' para o usu√°rio {user.id}. Usando apenas UTC.")
            pass 
            
        return datetime_context

    def _format_history_optimized(self, db_history: List[dict]) -> str:
        """Formata o hist√≥rico completo como texto estruturado (User/AI)."""
        formatted_lines = []
        for msg in db_history:
            role = "AI" if msg.get("role") == "assistant" else "User"
            content = msg.get("content", "").replace("\n", " ").strip()
            formatted_lines.append(f"{role}: {content}")
        return "\n".join(formatted_lines)

    async def generate_conversation_action(
        self,
        whatsapp: models.Atendimento,
        conversation_history_db: List[dict],
        persona: models.Config,
        db: AsyncSession,
        user: models.User
    ) -> dict:
        max_retries = 3
        last_response = None


        for attempt in range(max_retries):
            try:
                # 1. Coleta de Contexto
                system_instruction = persona.prompt or "Voc√™ √© um assistente √∫til."
                
                # Busca tags dispon√≠veis e atuais para o prompt
                available_tags = await crud_atendimento.get_all_user_tags(db, user.id)
                available_tags_names = [t['name'] for t in available_tags]
                
                current_tags_names = []
                if whatsapp.tags:
                     current_tags_names = [t['name'] for t in whatsapp.tags]

                # Gera o hist√≥rico formatado para o PROMPT
                history_str = self._format_history_optimized(conversation_history_db)
                
                # --- RAG QUERY BUILDER (Foco Exponencial) ---
                # Prioriza drasticamente as √∫ltimas mensagens para o embedding de busca.
                rag_query = ""
                if conversation_history_db:
                    # Pega as √∫ltimas 3 mensagens (Contexto Imediato)
                    recent_msgs = conversation_history_db[-5:]
                    rag_query = self._format_history_optimized(recent_msgs)


                # Usa a query focada para buscar contexto
                rag_context = await self._retrieve_rag_context(db, persona.id, rag_query)

                # --- Contexto de Data e Hora ---
                datetime_context = self._get_datetime_context(user)

                # 2. Montagem do Prompt (Texto Estruturado)
                prompt_text = (
                    f"# CONTEXTO (RAG)\n{rag_context}\n\n"
                    f"# DATA E HORA ATUAL\n{datetime_context}\n"
                    f"# HIST√ìRICO\n{history_str}\n\n"
                    f"# DADOS DO CLIENTE\n"
                    f"Nome: {whatsapp.nome_contato or 'N√£o identificado'}\n"
                    f"Tags Atuais: {json.dumps(current_tags_names, ensure_ascii=False)}\n"
                    f"Status Atendente: {'online' if user.atendente_online else 'offline'}\n\n"
                    f"# TAGS DISPON√çVEIS\n"
                    f"{json.dumps(available_tags_names, ensure_ascii=False)}\n\n"
                    f"# DIRETRIZES DE HUMANIZA√á√ÉO (CR√çTICO)\n"
                    f"- **Zero 'Corporatiqu√™s':** PROIBIDO come√ßar frases com '√ìtimo', 'Excelente', 'Perfeito', 'Entendido', 'Compreendo'. Isso denuncia que voc√™ √© um rob√¥. V√° direto ao ponto.\n"
                    f"- **N√ÉO SE REPITA (REGRA CR√çTICA):** Analise o hist√≥rico. √â PROIBIDO repetir informa√ß√µes, perguntas, a√ß√µes ou parafrasear o que o usu√°rio disse. Se voc√™ j√° deu uma informa√ß√£o, n√£o a d√™ novamente. Se o cliente aponta um erro seu, corrija o erro diretamente, sem reiniciar o fluxo de recomenda√ß√£o.\n"
                    f"- **Continuidade Real:** Trate o hist√≥rico como uma conversa cont√≠nua de WhatsApp. Se j√° houver mensagens anteriores, JAMAIS use 'Ol√°' ou apresenta√ß√µes novamente. Aja como se tivesse respondido h√° 1 minuto.\n"
                    f"- **Zero Sauda√ß√µes Repetidas:** Se j√° houve um cumprimento no hist√≥rico recente, N√ÉO inicie a resposta com 'Ol√°', 'Oi', 'Bom dia', etc. Continue a conversa diretamente.\n"
                    f"- **Conex√£o L√≥gica:** Use conectivos de conversa real ('Ent√£o...', 'Nesse caso...', 'Ah, sobre isso...'). Evite listas com bullets se puder responder em uma frase corrida.\n"
                    f"- **Espelhamento de Tom:** Se a mensagem do cliente for curta (ex: 'qual o pre√ßo?'), seja direto ('Custa R$ 50,00'). Se ele for detalhista, explique mais. N√£o escreva um 'text√£o' para quem perguntou 'sim ou n√£o'.\n"
                    f"- **Formata√ß√£o de Chat:** Evite listas com marcadores (bullets) ou negrito excessivo a menos que seja estritamente necess√°rio (como uma lista de itens). No WhatsApp, pessoas usam par√°grafos curtos, n√£o t√≥picos de Powerpoint.\n"
                    f"- **Banalidade Controlada:** Em vez de 'Sinto muito pelo inconveniente causado', use algo mais leve como 'Poxa, entendo o problema' ou 'Que chato isso, vamos resolver'. Evite desculpas exageradas e submissas.\n"
                    f"- **Proibido Repetir Nomes:** Use o nome do cliente APENAS na primeira sauda√ß√£o do dia. Nas mensagens seguintes, JAMAIS comece com 'Ah, {whatsapp.nome_contato}', 'Ol√° {whatsapp.nome_contato}' ou similares. Fale direto.\n"
                    f"- **Zero Interjei√ß√µes Artificiais:** N√£o comece frases com 'Ah, entendo!', 'Compreendo perfeitamente', 'Excelente pergunta'. Isso soa falso. V√° direto para a resposta t√©cnica/comercial.\n"
                    f"- **Par√°grafos √önicos:** Tente responder tudo em UM ou TRES par√°grafos no m√°ximo. Evite quebrar a resposta em v√°rias linhas curtas para n√£o gerar spam de notifica√ß√µes.\n\n"
                    f"# TAREFA\n"
                    f"Responda ao √∫ltimo 'User' agindo estritamente como a persona definida.\n\n"
                    f"# REGRAS DE EXECU√á√ÉO\n"
                    f"1. **Fonte de Verdade:** Use prioritariamente o CONTEXTO (RAG). Se n√£o encontrar, use conhecimento geral sensato, mas evite alucinar dados t√©cnicos.\n"
                    f"2. **Arquivos:** Se o cliente pedir foto/cat√°logo e o arquivo estiver listado no RAG, inclua-o em `arquivos_anexos` usando o ID exato. **PROIBIDO** colocar informa√ß√µes da imagem, links ou IDs no texto (`mensagem_para_enviar`).\n"
                    f"3. **Encaminhamento:** Tente resolver ao m√°ximo. Insista na resolu√ß√£o antes de sugerir um humano. Antes de encaminhar, SEMPRE pergunte se o cliente deseja falar com um atendente. S√≥ mude `nova_situacao` para 'Atendente Chamado' ap√≥s a confirma√ß√£o expl√≠cita do cliente.\n"
                    f"4. **Comunica√ß√£o:** Seja direto e use *negrito* para destaques. A regra de N√ÉO REPETIR √© a mais importante de todas.\n"
                    f"5. **Fluxo:** O sistema envia o texto PRIMEIRO e os arquivos DEPOIS. Considere isso na sua resposta.\n"
                    f"6. **Tags:** Analise a conversa e veja se alguma tag dispon√≠vel se aplica. Retorne apenas o nome das tags em `tags_sugeridas` para adicionar (ou null).\n"
                    f"7. **Capacidade de Vis√£o:** Voc√™ TEM a capacidade de ver e analisar imagens, v√≠deos, √°udios e documentos (PDFs) enviados pelo usu√°rio. Se o hist√≥rico mostrar '[Imagem/Doc Transcrito]', trate como se tivesse visto o arquivo original.\n"
                    f"8. **Solicitar Nome:** Se o nome do cliente estiver 'N√£o identificado', pergunte o nome dele logo no in√≠cio da intera√ß√£o.\n\n"
                    f"# FORMATO DE RESPOSTA (JSON OBRIGAT√ìRIO)\n"
                    f"Retorne APENAS um JSON v√°lido, sem blocos de c√≥digo (```json).\n"
                    f"{{\n"
                    f'  "mensagem_para_enviar": "Texto da resposta aqui (ou null)",\n'
                    f'  "nova_situacao": "Aguardando Resposta" | "Atendente Chamado" | "Conclu√≠do",\n'
                    f'  "nome_contato": "Nome extra√≠do ou null",\n'
                    f'  "tags_sugeridas": ["Tag1", "Tag2"] | null,\n'
                    f'  "resumo": "Resumo curto da conversa inteira para CRM",\n'
                    f'  "arquivos_anexos": [\n'
                    f'    {{ "nome_exato": "nome.pdf", "id_arquivo": "ID_DO_RAG", "tipo_midia": "image" }}\n'
                    f'  ]\n'
                    f"}}"
                )
                
                response = await self._generate_with_retry(prompt_text, db, user, system_instruction=system_instruction, atendimento_id=whatsapp.id)
                last_response = response
                
                return self._parse_json_response(response.text)

            except json.JSONDecodeError as e:
                response_text = last_response.text if last_response else "N/A"
                logger.warning(
                    f"Falha ao decodificar JSON da IA (tentativa {attempt + 1}/{max_retries}). "
                    f"Resposta: {response_text}"
                )
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)  # Aguarda antes da pr√≥xima tentativa
                else:
                    logger.error(f"Erro de decodifica√ß√£o JSON ap√≥s {max_retries} tentativas. Resposta final: {response_text}", exc_info=True)
                    return { "mensagem_para_enviar": None, "nova_situacao": "Erro IA", "resumo": f"Falha da IA ao gerar JSON v√°lido ap√≥s {max_retries} tentativas: {str(e)}" }
            
            except Exception as e:
                logger.error(f"Erro ao gerar a√ß√£o de conversa√ß√£o com Gemini: {e}", exc_info=True)
                return { "mensagem_para_enviar": None, "nova_situacao": "Erro IA", "resumo": f"Falha da IA: {str(e)}" }
        
        # Fallback caso o loop termine sem sucesso (n√£o deve acontecer com a l√≥gica acima)
        return { "mensagem_para_enviar": None, "nova_situacao": "Erro IA", "resumo": "Falha cr√≠tica no loop de gera√ß√£o de resposta da IA." }

    async def generate_followup_action(
        self,
        whatsapp: models.Atendimento,
        conversation_history_db: List[dict],
        db: AsyncSession,
        user: models.User
    ) -> dict:
        """
        Gera uma mensagem de follow-up baseada na inatividade e nas configura√ß√µes do usu√°rio.
        """
        try:
            history_str = self._format_history_optimized(conversation_history_db)
            datetime_context = self._get_datetime_context(user)

            prompt_text = (
                f"## TAREFA: FOLLOW-UP\n"
                f"Voc√™ √© um assistente especialista em reengajamento. Analise o hist√≥rico e decida se deve enviar um follow-up.\n\n"
                f"## CONTEXTO TEMPORAL\n{datetime_context}\n\n"
                f"## DADOS\nNome Contato: {whatsapp.nome_contato}\n\n"
                f"## HIST√ìRICO RECENTE\n{history_str}\n\n"
                f"## REGRAS\n"
                f"1. DECIS√ÉO DE ENVIO: A √∫nica condi√ß√£o para N√ÉO enviar mensagem √© se o cliente pediu explicitamente para parar ou n√£o ser mais contatado. Em todos os outros casos, voc√™ DEVE enviar o follow-up.\n"
                f"2. Se decidir enviar: Use a mensagem da configura√ß√£o como base, adaptando levemente para naturalidade. Seja curto, amig√°vel e n√£o insistente.\n"
                f"3. N√£o cumprimente novamente se j√° houver cumprimento no hist√≥rico.\n"
                f"4. Retorne APENAS um JSON v√°lido. Exemplo de formatos:\n"
                f"   - Para enviar: {{ \"action\": \"send\", \"mensagem_para_enviar\": \"texto...\" }}\n"
                f"   - Para n√£o enviar: {{ \"action\": \"skip\", \"mensagem_para_enviar\": null }}\n"
            )
            
            response = await self._generate_with_retry(prompt_text, db, user, atendimento_id=whatsapp.id)
            
            return self._parse_json_response(response.text)

        except Exception as e:
            logger.error(f"Erro ao gerar a√ß√£o de follow-up com Gemini: {e}", exc_info=True)
            return { "mensagem_para_enviar": None }

    def _format_analysis_json_to_markdown(self, analysis_data: Dict[str, Any]) -> str:
        """Converte o JSON de an√°lise da IA em uma string Markdown formatada."""
        markdown_parts = []

        # Extrai a chave principal, que pode variar (ex: 'analise_de_conversao')
        if not isinstance(analysis_data, dict):
            return str(analysis_data) # Retorna como string se n√£o for um dicion√°rio

        data = next(iter(analysis_data.values()), {}) if len(analysis_data) == 1 and isinstance(next(iter(analysis_data.values()), None), dict) else analysis_data

        if 'diagnostico_geral' in data:
            markdown_parts.append(f"## Diagn√≥stico Geral\n\n{data['diagnostico_geral']}\n")

        if 'principais_pontos_de_friccao' in data and data['principais_pontos_de_friccao']:
            markdown_parts.append("## Principais Pontos de Fric√ß√£o\n")
            for item in data['principais_pontos_de_friccao']:
                area = item.get('area') or item.get('ponto', '√Årea n√£o especificada')
                observacoes = item.get('observacoes') or item.get('detalhe', 'N/A')
                impacto = item.get('impacto_na_conversao')
                
                markdown_parts.append(f"### {area}")
                if impacto:
                    markdown_parts.append(f"**Impacto na Convers√£o:** {impacto}\n")
                markdown_parts.append(f"{observacoes}\n")

        if 'insights_acionaveis' in data and data['insights_acionaveis']:
            markdown_parts.append("## Insights Acion√°veis e Sugest√µes\n")
            for insight in data['insights_acionaveis']:
                markdown_parts.append(f"### {insight.get('titulo', 'Sugest√£o')}\n")
                for sugestao in insight.get('sugestoes', []):
                    markdown_parts.append(f"- {sugestao}")
                markdown_parts.append("") # Adiciona uma linha em branco

        if 'proximos_passos_recomendados' in data:
            markdown_parts.append(f"## Pr√≥ximos Passos\n\n{data['proximos_passos_recomendados']}")

        if not markdown_parts: # Fallback se a estrutura for inesperada
            return "A an√°lise foi gerada, mas em um formato n√£o esperado para formata√ß√£o autom√°tica."

        return "\n".join(markdown_parts)

    async def analyze_data(
        self,
        question: str,
        user: models.User,
        atendimentos: List[models.Atendimento], # Lista de atendimentos do per√≠odo
        persona: Optional[models.Config],       # A persona padr√£o
        db: AsyncSession,
    ) -> Dict[str, Any]:
        """
        Usa a IA para analisar dados do sistema com base em uma pergunta do usu√°rio.
        Retorna um dicion√°rio JSON com a an√°lise estruturada.
        """
        logger.info(f"Iniciando an√°lise de dados para user_id={user.id} com a pergunta: '{question[:100]}...'")

        # 1. System Instruction
        system_instruction = (
            "Voc√™ √© um analista de dados s√™nior especialista em atendimento ao cliente.\n"
            "Sua tarefa √© analisar os dados fornecidos e responder √† pergunta do usu√°rio.\n"
            "Sua resposta DEVE ser estritamente um objeto JSON v√°lido, sem markdown de c√≥digo.\n"
            "Siga a estrutura sugerida para organizar sua an√°lise."
        )

        # 2. Processamento dos dados quantitativos (Estat√≠sticas Gerais)
        total = len(atendimentos)
        status_counts = {}
        for at in atendimentos:
            status_counts[at.status] = status_counts.get(at.status, 0) + 1
        
        stats_summary = {
            "total_atendimentos": total,
            "distribuicao_status": status_counts,
            "periodo_analisado": "Verificar datas nos filtros"
        }

        # 3. RAG em Mem√≥ria para dados qualitativos (Conversas/Observa√ß√µes)
        # Prepara textos para embedding (Limitado aos 100 mais recentes para performance)
        docs_for_embedding = []
        atendimentos_map = {} 
        
        # Ordena por data de atualiza√ß√£o (mais recentes primeiro) se ainda n√£o estiver
        sorted_atendimentos = sorted(atendimentos, key=lambda x: x.updated_at, reverse=True)[:100]

        for idx, at in enumerate(sorted_atendimentos):
            conversa_text = ""
            try:
                msgs = json.loads(at.conversa or "[]")
                # Pega as √∫ltimas 5 mensagens para contexto
                last_msgs = msgs[-5:]
                conversa_text = " | ".join([f"{m.get('role')}: {m.get('content')}" for m in last_msgs])
            except:
                conversa_text = "Sem hist√≥rico leg√≠vel."

            doc_text = (
                f"Status: {at.status}. "
                f"Resumo: {at.resumo or ''}. "
                f"Conversa recente: {conversa_text}"
            )
            docs_for_embedding.append(doc_text)
            atendimentos_map[idx] = at

        relevant_atendimentos_data = []
        
        if docs_for_embedding and question:
            try:
                q_embedding = await self.generate_embedding(question)
                if q_embedding:
                    doc_embeddings = await self.generate_embeddings_batch(docs_for_embedding)
                    
                    scores = []
                    q_vec = np.array(q_embedding)
                    norm_q = np.linalg.norm(q_vec)

                    for d_vec in doc_embeddings:
                        if not d_vec:
                            scores.append(-1)
                            continue
                        d_vec_np = np.array(d_vec)
                        norm_d = np.linalg.norm(d_vec_np)
                        if norm_q == 0 or norm_d == 0:
                            scores.append(0)
                        else:
                            scores.append(np.dot(q_vec, d_vec_np) / (norm_q * norm_d))
                    
                    # Seleciona Top 15 mais relevantes
                    top_indices = np.argsort(scores)[::-1][:15]
                    
                    for idx in top_indices:
                        if scores[idx] > 0.25: # Threshold de relev√¢ncia
                            at = atendimentos_map[idx]
                            relevant_atendimentos_data.append({
                                "id": at.id,
                                "nome": at.nome_contato,
                                "status": at.status,
                                "resumo": at.resumo,
                                "trecho_conversa": docs_for_embedding[idx]
                            })
            except Exception as e:
                logger.error(f"Erro no RAG do Dashboard: {e}")

        persona_context = None
        if persona:
            persona_context = {"nome_persona": persona.nome_config, "contexto": persona.prompt}

        analysis_prompt = {
            "pergunta_usuario": question,
            "dados_estatisticos": stats_summary,
            "dados_qualitativos_relevantes": relevant_atendimentos_data,
            "contexto_adicional": {
                "resumo_usuario": {"id": user.id, "email": user.email, "tokens_restantes": user.tokens},
                "contexto_persona_ia": persona_context or "N/A",
            },
            "instrucoes_formato": {
                "analise_de_conversao": {
                    "diagnostico_geral": "Um par√°grafo resumindo a situa√ß√£o.",
                    "principais_pontos_de_friccao": [
                        {"area": "Nome da √Årea (ex: Pre√ßos)", "observacoes": "Detalhes observados em texto simples.", "impacto_na_conversao": "Alto/M√©dio/Baixo"}
                    ],
                    "insights_acionaveis": [
                        {"titulo": "T√≠tulo da Sugest√£o", "sugestoes": ["Sugest√£o 1 em texto simples.", "Sugest√£o 2 em texto simples."]}
                    ],
                    "proximos_passos_recomendados": "Recomenda√ß√£o final."
                }
            }
        }

        prompt_str = json.dumps(analysis_prompt, ensure_ascii=False, indent=2)

        response = await self._generate_with_retry(
            prompt_str, 
            db, 
            user, 
            is_media=False, 
            system_instruction=system_instruction
        )
        return self._parse_json_response(response.text)

    async def generate_persona_prompt(
        self,
        company_description: str,
        db: AsyncSession,
        user: models.User
    ) -> str:
        """Gera um prompt de sistema (persona) com base na descri√ß√£o da empresa e instru√ß√µes."""
        prompt_text = (
            f"Atue como um Especialista em Engenharia de Prompt e Estrategista de Vendas.\n"
            f"Seu objetivo √© escrever um 'System Prompt' (Persona) para um assistente de WhatsApp que deve parecer 100% humano.\n\n"
            f"DIRETRIZES PARA O PROMPT GERADO:\n"
            f"1. TOM E ESTILO: A linguagem deve ser natural, amig√°vel e t√≠pica de WhatsApp (mensagens curtas, uso de emojis, sem formalismo excessivo). Proibido usar frases como 'em que posso ajudar hoje?' ou termos rob√≥ticos. Proibido admitir que √© uma IA.\n"
            f"2. ABERTURA OBRIGAT√ìRIA: O assistente deve sempre iniciar a primeira intera√ß√£o dizendo: 'Ol√°! Sou o [Nome] da [Nome da Empresa]. Com quem eu falo?' para obter o nome do cliente.\n"
            f"3. CRIATIVIDADE EM PRODUTOS/PRE√áOS: Se a descri√ß√£o da empresa n√£o fornecer uma lista detalhada, a IA deve inventar produtos, servi√ßos e pre√ßos condizentes com o nicho, apresentando-os com total naturalidade e autoridade, como se fizessem parte do cat√°logo oficial.\n"
            f"4. FLUXO DE CONVERSA: O atendimento deve ser focado em entender a necessidade do cliente, oferecer as op√ß√µes geradas e conduzir para o fechamento ou agendamento, dependendo do nicho.\n"
            f"5. REGRAS: Nunca quebrar o personagem. Se o cliente perguntar algo muito fora do escopo, o assistente deve contornar de forma humana (ex: 'vou confirmar essa informa√ß√£o com o financeiro').\n\n"
            f"Informa√ß√µes para basear a cria√ß√£o:\n{company_description}\n\n"
            f"Retorne APENAS o texto do system prompt gerado, sem introdu√ß√µes, sem formata√ß√£o markdown (```) e pronto para uso."
        )
        
        response = await self._generate_with_retry(prompt_text, db, user)
        
        # L√≥gica fixa de Teste Gratuito (Adicionada conforme solicita√ß√£o)
        fixed_trial_prompt = (
            "\n\n"
            "--- MUDAN√áA DE FASE: CONSULTORIA P√ìS-DEMONSTRA√á√ÉO ---\n"
            "1. GATILHO DE ATIVA√á√ÉO: Verifique o hist√≥rico. Se a contagem total de mensagens (User + AI) for maior ou igual a 8.\n"
            "Voc√™ deve encerrar a atua√ß√£o da persona e assumir a identidade de 'Consultor Especialista da Digital for Me'.\n\n"

            "2. TRANSI√á√ÉO E META-EXPLICA√á√ÉO (Primeira Mensagem):\n"
            "   - Explique gentilmente que o limite de intera√ß√µes do teste gratuito foi atingido.\n"
            "   - CLAREZA: Informe explicitamente que o atendimento anterior (Aurora) foi uma simula√ß√£o com dados fict√≠cios para demonstrar a capacidade da IA de manter conversas naturais.\n"
            "   - OBJETIVO: Pergunte o que ele achou da experi√™ncia e se ele consegue visualizar essa tecnologia atendendo os clientes dele.\n\n"

            "3. DIRETRIZES DE COMPORTAMENTO (Consultor S√™nior):\n"
            "   - Tom de Voz: Profissional, parceiro de neg√≥cios, seguro e emp√°tico. N√£o seja rob√≥tico.\n"
            "   - Foco: Voc√™ n√£o est√° apenas vendendo um software, est√° vendendo 'Tempo' e 'Qualifica√ß√£o de Leads'.\n"
            "   - Dados da Empresa (Use para gerar autoridade): Somos a Digital for Me, sediados em Toledo-PR. Somos reais e especialistas em automa√ß√£o.\n\n"

            "4. REGRA DE OURO DOS LINKS (Anti-Spam):\n"
            "   - JAMAIS termine toda mensagem com o link de compra.\n"
            "   - Quando enviar o link: APENAS se o usu√°rio perguntar explicitamente sobre 'pre√ßo', 'como contratar' ou 'planos', ou ap√≥s recomendar um plano espec√≠fico.\n"
            "   - Links de Pagamento (PayPal) e Detalhes dos Planos:\n"
            "       * CRM Start (R$ 97,00/M√™s): https://www.paypal.com/webapps/billing/plans/subscribe?plan_id=P-8CH598740K3040248NGB2CRA\n"
            "         (Inclui: Plataforma CRM Completa, Gest√£o de Funil de Vendas, Integra√ß√£o WhatsApp Oficial, Cr√©ditos para Teste de IA, Suporte IA)\n"
            "       * Plano Essencial (R$ 297,00/M√™s): https://www.paypal.com/webapps/billing/plans/subscribe?plan_id=P-1XB92324FX039614ENGB2DEA\n"
            "         (Inclui: Atendimento Humanizado Ativo, 50% Revers√£o em Cr√©ditos, Integra√ß√£o WhatsApp Oficial, Plataforma CRM Completa, Configura√ß√£o Guiada Inclusa)\n"
            "       * Plano Domin√¢ncia (R$ 597,00/M√™s): https://www.paypal.com/webapps/billing/plans/subscribe?plan_id=P-24K680067A731431ENGB2DSI\n"
            "         (Inclui: Tudo do Plano Essencial, Acesso antecipado Prospect AI, Prospec√ß√£o de Novos Leads, 100% Revers√£o em Cr√©ditos, Suporte Priorit√°rio)\n"
            "       * Plano Elite Studio (A Partir de R$ 997,00/M√™s): https://www.paypal.com/webapps/billing/plans/subscribe?plan_id=P-3VJ82116B1880631XNGB2D5Y\n"
            "         (Inclui: Voz Ativa em Futuros Desenvolvimentos, Personaliza√ß√µes sob Demanda, Engenharia de Prompt Dedicada, Estrat√©gias de Alta Escala, 100% Revers√£o em Cr√©ditos, Acesso aos Bastidores)\n\n"

            "5. ARGUMENTA√á√ÉO E QUEBRA DE OBJE√á√ïES:\n"
            "   - Custo vs. Investimento: Se o cliente achar caro, compare com o custo de um funcion√°rio humano (sal√°rio + encargos + risco trabalhista + limita√ß√£o de hor√°rio). A IA trabalha 24/7 sem encargos.\n"
            "   - Personaliza√ß√£o: Garanta que a IA ser√° treinada especificamente para o neg√≥cio dele (Arquiteta de Solu√ß√µes), n√£o √© um rob√¥ gen√©rico.\n\n"

            "6. QUALIFICA√á√ÉO E FECHAMENTO:\n"
            "   - Antes de empurrar um plano, fa√ßa uma pergunta diagn√≥stica: 'Qual a m√©dia de atendimentos/leads que sua empresa recebe por m√™s?'\n"
            "   - Recomenda√ß√£o:\n"
            "       * At√© 100 atendimentos -> Indique o Plano Essencial.\n"
            "       * At√© 500 atendimentos -> Indique o Plano Domin√¢ncia.\n"
            "       * Acima de 1000 -> Indique o Plano Elite Studio.\n"
            "   - Ao recomendar o plano, envie o link de pagamento correspondente listado na se√ß√£o 4.\n\n"

            "7. FALLBACK (Transbordo Humano):\n"
            "   - Se houver uma d√∫vida t√©cnica complexa que voc√™ n√£o saiba responder com certeza, mude a situa√ß√£o para 'Atendente Chamado', aonde um humano ir√° atender.\n"
        )

        return response.text.strip() + fixed_trial_prompt

_gemini_service_instance = None
def get_gemini_service():
    global _gemini_service_instance
    if _gemini_service_instance is None:
        _gemini_service_instance = GeminiService()
    return _gemini_service_instance