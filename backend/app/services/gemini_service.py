from google import genai
from google.genai import types
import logging
import json
import asyncio
import re
from typing import Optional, List, Dict, Any
from collections.abc import Set
import numpy as np

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select


from app.core.config import settings
from app.db import models
from app.crud import crud_user

logger = logging.getLogger(__name__)

class SetEncoder(json.JSONEncoder):
    """Codificador JSON para lidar com objetos 'set'."""
    def default(self, obj):
        if isinstance(obj, Set):
            return list(obj)
        return super().default(obj)

class GeminiService:
    def __init__(self):
        keys_str = settings.GOOGLE_API_KEYS
        self.api_keys = [key.strip() for key in keys_str.split(',') if key.strip()]

        if not self.api_keys:
            logger.error("üö® ERRO CR√çTICO: Nenhuma chave de API do Google foi configurada em GOOGLE_API_KEYS.")
            raise ValueError("A lista de GOOGLE_API_KEYS n√£o pode estar vazia.")
            
        self.current_key_index = 0
        self.generation_config = {"temperature": 0.5, "top_p": 1, "top_k": 1}
        
        self._initialize_model()

    def _initialize_model(self):
        """Inicializa o cliente Gemini com a chave atual usando o novo SDK."""
        try:
            current_key = self.api_keys[self.current_key_index]
            
            # NOVO SDK: Instancia o Client
            # http_options={'api_version': 'v1alpha'} pode ser usado se precisar de recursos beta
            self.client = genai.Client(api_key=current_key)
            
            logger.info(f"‚úÖ Cliente Gemini (New SDK) inicializado com sucesso (chave √≠ndice {self.current_key_index}).")
        except Exception as e:
            logger.error(f"üö® ERRO CR√çTICO ao configurar o Gemini com a chave √≠ndice {self.current_key_index}: {e}", exc_info=True)
            raise

    def _rotate_key(self):
        """Muda para a pr√≥xima chave na lista."""
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        logger.warning(f"Alternando para a chave de API do Google com √≠ndice {self.current_key_index}.")
        self._initialize_model()
        return self.current_key_index

    async def _generate_with_retry(
        self, 
        prompt: Any, 
        db: AsyncSession, 
        user: models.User, 
        is_media: bool = False,
        system_instruction: Optional[str] = None
    ):  # Removido o tipo de retorno estrito para evitar erros de importa√ß√£o cruzada por enquanto
        """
        Executa a chamada para a API Gemini (Novo SDK), deduz token e rotaciona chaves.
        """
        
        # Configura√ß√£o do novo SDK
        # Adaptamos o dicion√°rio antigo para o novo objeto de configura√ß√£o
        config_args = {
            "temperature": self.generation_config.get("temperature", 0.5),
            "top_p": self.generation_config.get("top_p", 1),
            "top_k": self.generation_config.get("top_k", 1),
        }

        if not is_media and isinstance(prompt, str):
            logger.debug(f"Prompt (texto) para a IA: {prompt[:500]}...")
            # No novo SDK, response_mime_type entra na config
            config_args["response_mime_type"] = "application/json"

        # Adiciona system_instruction se fornecido
        if system_instruction:
            config_args["system_instruction"] = system_instruction

        # Cria o objeto de configura√ß√£o tipado
        gen_config = types.GenerateContentConfig(**config_args)

        # --- DEBUG: PRINT PROMPT ---
        try:
            debug_msg = f"\n{'='*20} PROMPT ENVIADO PARA IA {'='*20}\n"
            
            if system_instruction:
                debug_msg += f"--- SYSTEM INSTRUCTION ---\n{system_instruction}\n{'-'*30}\n"

            if isinstance(prompt, str):
                debug_msg += f"{prompt}\n"
            elif isinstance(prompt, list):
                for p in prompt:
                    if isinstance(p, str):
                        debug_msg += f"[TEXTO]: {p}\n"
                    else:
                        debug_msg += f"[M√çDIA/OBJETO]: {type(p)}\n"
            debug_msg += f"{'='*60}\n"
            
            # Salva o √∫ltimo prompt em arquivo (sobrescreve)
            with open("last_prompt.txt", "w", encoding="utf-8") as f:
                f.write(debug_msg)
        except Exception as e:
            print(f"Erro ao printar/salvar prompt: {e}")

        initial_key_index = self.current_key_index
        max_attempts_per_key = 2
        
        while True:
            for attempt in range(max_attempts_per_key):
                try:
                    logger.info(
                        f"Tentando gerar conte√∫do com a chave √≠ndice {self.current_key_index} "
                        f"(tentativa {attempt + 1}/{max_attempts_per_key})."
                    )
                    
                    # --- MUDAN√áA PRINCIPAL: Chamada Ass√≠ncrona Nativa (.aio) ---
                    # N√£o precisa mais de run_in_executor
                    response = await self.client.aio.models.generate_content(
                        model='gemini-2.5-flash', # Modelo corrigido para vers√£o est√°vel
                        contents=prompt,
                        config=gen_config
                    )
                    
                    # --- L√ìGICA DE TOKEN (OD√îMETRO) ---
                    # Extrai o uso real de tokens da resposta do Gemini
                    usage_metadata = response.usage_metadata
                    total_tokens = usage_metadata.total_token_count if usage_metadata else 0

                    logger.info(f"Sucesso na chamada √† API Gemini para o utilizador {user.id}. Deduzindo {total_tokens} tokens.")
                    try:
                        await crud_user.decrement_user_tokens(db, db_user=user, usage=total_tokens)
                        await db.commit()
                        await db.refresh(user)
                    except Exception as token_err:
                        logger.error(f"Falha ao deduzir tokens: {token_err}", exc_info=True)
                        await db.rollback()
                    
                    return response

                # Captura erros do novo SDK (geralmente ServerError ou ClientError)
                # O erro 429 (Quota) agora geralmente vem como um ClientError com status 429
                except Exception as e:
                    error_str = str(e).lower()
                    
                    # Detec√ß√£o de Erro de Cota (429) ou Recurso Esgotado
                    if "429" in error_str or "resource exhausted" in error_str or "quota" in error_str:
                        logger.warning(f"Quota da API excedida (429) com a chave {self.current_key_index}. Rotacionando...")
                        break # Sai do loop 'for' para rotacionar a chave
                    
                    # Detec√ß√£o de bloqueio de seguran√ßa ou prompt inv√°lido
                    elif "blocked" in error_str or "invalid argument" in error_str:
                        logger.error(f"Erro n√£o recuper√°vel (Bloqueio/Inv√°lido): {e}")
                        raise e
                        
                    else:
                        # Erros gen√©ricos de conex√£o/servidor
                        logger.error(f"Erro inesperado na API Gemini: {e}. Tentativa {attempt + 1}.")
                        await asyncio.sleep(2) # Espera um pouco antes de tentar de novo na mesma chave
            
            # Se saiu do loop 'for', significa que precisa trocar de chave
            new_key_index = self._rotate_key()
            
            if new_key_index == initial_key_index:
                logger.critical(f"Todas as {len(self.api_keys)} chaves de API falharam.")
                raise Exception("Todas as chaves de API excederam a quota.")

    async def transcribe_and_analyze_media(
        self, 
        media_data: dict,  # Espera receber: {"data": bytes, "mime_type": str}
        db_history: List[dict], 
        persona: models.Config,
        db: AsyncSession,
        user: models.User
    ) -> str:
        logger.info(f"Iniciando transcri√ß√£o/an√°lise para m√≠dia do tipo {media_data.get('mime_type')}")
        
        # --- 1. PREPARA√á√ÉO DA M√çDIA PARA O NOVO SDK ---
        try:
            file_bytes = media_data.get("data")
            mime_type = media_data.get("mime_type")

            if not file_bytes:
                raise ValueError("Bytes do arquivo n√£o encontrados em media_data")

            # Cria o objeto Part nativo do novo SDK
            # Isso substitui a l√≥gica antiga de upload ou passagem de objetos complexos
            media_part = types.Part.from_bytes(
                data=file_bytes, 
                mime_type=mime_type
            )
        except Exception as e:
            logger.error(f"Erro ao preparar objeto de m√≠dia para o Gemini: {e}")
            return "[Erro interno ao processar o arquivo de m√≠dia]"

        # --- 2. MONTAGEM DO PROMPT (Lista de conte√∫dos) ---
        prompt_contents = []
        
        # L√≥gica para √Åudio (Transcri√ß√£o)
        if 'audio' in mime_type or 'mpeg' in mime_type or 'ogg' in mime_type:
            task_text = "Sua √∫nica tarefa √© transcrever o √°udio a seguir. Retorne apenas o texto transcrito, sem adicionar nenhuma outra palavra, introdu√ß√£o ou formata√ß√£o."
            prompt_contents = [task_text, media_part]
            
        # L√≥gica para Imagem/Documento (An√°lise Visual)
        else:
            system_instruction = persona.prompt or "Voc√™ √© um especialista em extra√ß√£o de dados."
            
            last_user_msg = next((m.get('content', '') for m in reversed(db_history) if m.get('role') == 'user'), "")
            rag_context = await self._retrieve_rag_context(db, persona.id, last_user_msg)
            
            history_str = self._format_history_optimized(db_history)
            
            prompt_text = (
                f"## CONTEXTO (RAG)\n{rag_context}\n\n"
                f"## HIST√ìRICO RECENTE\n{history_str}\n\n"
                "## INSTRU√á√ÉO DE AN√ÅLISE\n"
                "Voc√™ √© um especialista em extra√ß√£o de dados. Analise o arquivo fornecido.\n"
                "1. Extraia todos os dados vis√≠veis e relevantes (pre√ßos, produtos, nomes, endere√ßos).\n"
                "2. Se for um comprovante, extraia valor, data e benefici√°rio.\n"
                "3. N√£o converse. Apenas retorne os dados extra√≠dos em texto claro.\n"
                "4. Use o contexto e hist√≥rico acima para entender o que buscar."
            )
            
            # Ordem: Prompt de texto primeiro, M√≠dia depois (ou vice-versa, Gemini entende ambos)
            prompt_contents = [prompt_text, media_part]

        # --- 3. CHAMADA √Ä API ---
        try:
            # Passamos a lista (texto + m√≠dia) para o m√©todo que criamos anteriormente
            # O _generate_with_retry j√° est√° preparado para receber 'prompt' como string OU lista
            response = await self._generate_with_retry(prompt_contents, db, user, is_media=True, system_instruction=system_instruction)
            
            transcription = response.text.strip()
            logger.info(f"Transcri√ß√£o/An√°lise gerada: '{transcription[:100]}...'")
            return transcription
            
        except Exception as e:
            logger.error(f"Erro ao transcrever/analisar m√≠dia com Gemini: {e}", exc_info=True)
            return f"[Erro ao processar m√≠dia: {mime_type}]"

    async def generate_embedding(self, text: str) -> List[float]:
        """Gera embedding para um texto usando o modelo do Google (text-embedding-004)."""
        try:
            # O novo SDK usa client.aio.models.embed_content
            response = await self.client.aio.models.embed_content(
                model="text-embedding-004",
                contents=text
            )
            if response.embeddings:
                return response.embeddings[0].values
            return []
        except Exception as e:
            logger.error(f"Erro ao gerar embedding: {e}")
            return []

    async def generate_embeddings_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """Gera embeddings para uma lista de textos em lotes (batching)."""
        all_embeddings = []
        
        # Divide a lista total em peda√ßos menores (chunks) para respeitar limites da API
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            try:
                # O novo SDK suporta lista de strings em 'contents' para processamento em lote
                response = await self.client.aio.models.embed_content(
                    model="text-embedding-004",
                    contents=batch
                )
                
                if response.embeddings:
                    # Extrai os valores de cada embedding retornado, mantendo a ordem
                    batch_embeddings = [e.values for e in response.embeddings]
                    all_embeddings.extend(batch_embeddings)
                else:
                    logger.warning(f"Batch {i} retornou sem embeddings.")
                    all_embeddings.extend([[] for _ in batch])

            except Exception as e:
                logger.error(f"Erro ao gerar embeddings em lote (√≠ndice {i}): {e}")
                # Adiciona listas vazias para n√£o quebrar o alinhamento dos √≠ndices com os textos originais
                all_embeddings.extend([[] for _ in batch])
        
        return all_embeddings

    async def _retrieve_rag_context(self, db: AsyncSession, config_id: int, query_text: str) -> str:
        """Busca contexto relevante na base vetorial (PGVector) usando similaridade de cosseno."""
        if not query_text: return ""
        
        # 1. Gera o embedding da pergunta do usu√°rio
        query_embedding = await self.generate_embedding(query_text)
        
        if not query_embedding:
            logger.warning("Falha ao gerar embedding da query. Retornando vazio.")
            return ""

        # 2. Busca vetorial ordenada pela dist√¢ncia de cosseno
        stmt = select(models.KnowledgeVector.content).where(
            models.KnowledgeVector.config_id == config_id
        ).order_by(
            models.KnowledgeVector.embedding.cosine_distance(query_embedding)
        ).limit(10) # Busca os 10 trechos mais relevantes
        
        result = await db.execute(stmt)
        chunks = result.scalars().all()
        
        if not chunks: return ""
        
        # Remove duplicatas exatas de conte√∫do
        unique_chunks = list(dict.fromkeys(chunks))
        
        return "\n---\n".join(unique_chunks)

    def _format_history_optimized(self, db_history: List[dict]) -> str:
        """Formata o hist√≥rico como texto estruturado (User/AI) com janela deslizante de 10 msgs."""
        recent_messages = db_history[-10:]
        formatted_lines = []
        for msg in recent_messages:
            role = "AI" if msg.get("role") == "assistant" else "User"
            content = msg.get("content", "").replace("\n", " ").strip()
            formatted_lines.append(f"{role}: {content}")
        return "\n".join(formatted_lines)

    async def generate_conversation_action(
        self,
        whatsapp: models.Atendimento,
        conversation_history_db: List[dict],
        persona: models.Config,
        db: AsyncSession,
        user: models.User
    ) -> dict:
        max_retries = 3
        last_response = None


        for attempt in range(max_retries):
            try:
                # 1. Coleta de Contexto
                system_instruction = persona.prompt or "Voc√™ √© um assistente √∫til."
                
                # Gera o hist√≥rico formatado (√∫ltimas 10 msgs) ANTES do RAG
                history_str = self._format_history_optimized(conversation_history_db)
                
                # Usa o hist√≥rico completo (formatado) para buscar contexto (Sheets + Drive)
                rag_context = await self._retrieve_rag_context(db, persona.id, history_str)

                # 2. Montagem do Prompt (Texto Estruturado)
                prompt_text = (
                    f"# CONTEXTO (RAG)\n{rag_context}\n\n"
                    f"# HIST√ìRICO\n{history_str}\n\n"
                    f"# DADOS DO CLIENTE\n"
                    f"Nome: {whatsapp.nome_contato or 'N√£o identificado'}\n"
                    f"Status Atendente: {'online' if user.atendente_online else 'offline'}\n\n"
                    f"# TAREFA\n"
                    f"Responda ao √∫ltimo 'User' agindo estritamente como a persona definida.\n\n"
                    f"# REGRAS DE EXECU√á√ÉO\n"
                    f"1. **Fonte de Verdade:** Use prioritariamente o CONTEXTO (RAG). Se n√£o encontrar, use conhecimento geral sensato, mas evite alucinar dados t√©cnicos.\n"
                    f"2. **Arquivos:** Se o cliente pedir foto/cat√°logo e o arquivo estiver listado no RAG, inclua-o em `arquivos_anexos` usando o ID exato. No texto, avise que est√° enviando.\n"
                    f"3. **Encaminhamento:** Tente resolver. S√≥ mude `nova_situacao` para 'Atendente Chamado' se for um caso complexo fora da base ou ap√≥s persist√™ncia do erro.\n"
                    f"4. **Comunica√ß√£o:** N√£o repita sauda√ß√µes (Oi/Ol√°) se j√° houver no hist√≥rico. Seja direto e use *negrito* para destaques.\n"
                    f"5. **Fluxo:** O sistema envia o texto PRIMEIRO e os arquivos DEPOIS. Considere isso na sua resposta.\n\n"
                    f"# FORMATO DE RESPOSTA (JSON OBRIGAT√ìRIO)\n"
                    f"Retorne APENAS um JSON v√°lido, sem blocos de c√≥digo (```json).\n"
                    f"{{\n"
                    f'  "mensagem_para_enviar": "Texto da resposta aqui (ou null)",\n'
                    f'  "nova_situacao": "Aguardando Resposta" | "Atendente Chamado" | "Conclu√≠do",\n'
                    f'  "nome_contato": "Nome extra√≠do ou null",\n'
                    f'  "observacoes": "Resumo curto para CRM",\n'
                    f'  "arquivos_anexos": [\n'
                    f'    {{ "nome_exato": "nome.pdf", "id_arquivo": "ID_DO_RAG", "tipo_midia": "image" }}\n'
                    f'  ]\n'
                    f"}}"
                )
                
                response = await self._generate_with_retry(prompt_text, db, user, system_instruction=system_instruction)
                last_response = response
                
                clean_response = response.text.strip().replace("```json", "").replace("```", "")
                
                return json.loads(clean_response)

            except json.JSONDecodeError as e:
                response_text = last_response.text if last_response else "N/A"
                logger.warning(
                    f"Falha ao decodificar JSON da IA (tentativa {attempt + 1}/{max_retries}). "
                    f"Resposta: {response_text}"
                )
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)  # Aguarda antes da pr√≥xima tentativa
                else:
                    logger.error(f"Erro de decodifica√ß√£o JSON ap√≥s {max_retries} tentativas. Resposta final: {response_text}", exc_info=True)
                    return { "mensagem_para_enviar": None, "nova_situacao": "Erro IA", "observacoes": f"Falha da IA ao gerar JSON v√°lido ap√≥s {max_retries} tentativas: {str(e)}" }
            
            except Exception as e:
                logger.error(f"Erro ao gerar a√ß√£o de conversa√ß√£o com Gemini: {e}", exc_info=True)
                return { "mensagem_para_enviar": None, "nova_situacao": "Erro IA", "observacoes": f"Falha da IA: {str(e)}" }
        
        # Fallback caso o loop termine sem sucesso (n√£o deve acontecer com a l√≥gica acima)
        return { "mensagem_para_enviar": None, "nova_situacao": "Erro IA", "observacoes": "Falha cr√≠tica no loop de gera√ß√£o de resposta da IA." }

    async def generate_followup_action(
        self,
        whatsapp: models.Atendimento,
        conversation_history_db: List[dict],
        followup_config: Dict[str, Any],
        db: AsyncSession,
        user: models.User
    ) -> dict:
        """
        Gera uma mensagem de follow-up baseada na inatividade e nas configura√ß√µes do usu√°rio.
        """
        try:
            history_str = self._format_history_optimized(conversation_history_db)

            prompt_text = (
                f"## TAREFA: FOLLOW-UP\n"
                f"Voc√™ √© um assistente de IA especialista em reengajamento. Gere uma mensagem de follow-up.\n\n"
                f"## CONFIGURA√á√ÉO DE FOLLOW-UP\n{json.dumps(followup_config, ensure_ascii=False)}\n\n"
                f"## DADOS\nNome Contato: {whatsapp.nome_contato}\n\n"
                f"## HIST√ìRICO RECENTE\n{history_str}\n\n"
                f"## REGRAS\n"
                f"1. Use a mensagem da configura√ß√£o como base, adaptando levemente para naturalidade.\n"
                f"2. Seja curto, amig√°vel e n√£o insistente.\n"
                f"3. N√£o cumprimente novamente se j√° houver cumprimento no hist√≥rico.\n"
                f"4. Retorne APENAS um JSON v√°lido: {{ \"mensagem_para_enviar\": \"texto...\" }}\n"
            )
            
            response = await self._generate_with_retry(prompt_text, db, user)
            
            clean_response = response.text.strip().replace("```json", "").replace("```", "")
            
            return json.loads(clean_response)

        except Exception as e:
            logger.error(f"Erro ao gerar a√ß√£o de follow-up com Gemini: {e}", exc_info=True)
            return { "mensagem_para_enviar": None }

    def _format_analysis_json_to_markdown(self, analysis_data: Dict[str, Any]) -> str:
        """Converte o JSON de an√°lise da IA em uma string Markdown formatada."""
        markdown_parts = []

        # Extrai a chave principal, que pode variar (ex: 'analise_de_conversao')
        if not isinstance(analysis_data, dict):
            return str(analysis_data) # Retorna como string se n√£o for um dicion√°rio

        data = next(iter(analysis_data.values()), {}) if len(analysis_data) == 1 and isinstance(next(iter(analysis_data.values()), None), dict) else analysis_data

        if 'diagnostico_geral' in data:
            markdown_parts.append(f"## Diagn√≥stico Geral\n\n{data['diagnostico_geral']}\n")

        if 'principais_pontos_de_friccao' in data and data['principais_pontos_de_friccao']:
            markdown_parts.append("## Principais Pontos de Fric√ß√£o\n")
            for item in data['principais_pontos_de_friccao']:
                area = item.get('area') or item.get('ponto', '√Årea n√£o especificada')
                observacoes = item.get('observacoes') or item.get('detalhe', 'N/A')
                impacto = item.get('impacto_na_conversao')
                
                markdown_parts.append(f"### {area}")
                if impacto:
                    markdown_parts.append(f"**Impacto na Convers√£o:** {impacto}\n")
                markdown_parts.append(f"{observacoes}\n")

        if 'insights_acionaveis' in data and data['insights_acionaveis']:
            markdown_parts.append("## Insights Acion√°veis e Sugest√µes\n")
            for insight in data['insights_acionaveis']:
                markdown_parts.append(f"### {insight.get('titulo', 'Sugest√£o')}\n")
                for sugestao in insight.get('sugestoes', []):
                    markdown_parts.append(f"- {sugestao}")
                markdown_parts.append("") # Adiciona uma linha em branco

        if 'proximos_passos_recomendados' in data:
            markdown_parts.append(f"## Pr√≥ximos Passos\n\n{data['proximos_passos_recomendados']}")

        if not markdown_parts: # Fallback se a estrutura for inesperada
            return "A an√°lise foi gerada, mas em um formato n√£o esperado para formata√ß√£o autom√°tica."

        return "\n".join(markdown_parts)

    async def analyze_data(
        self,
        question: str,
        user: models.User,
        atendimentos: List[models.Atendimento], # Lista de atendimentos do per√≠odo
        persona: Optional[models.Config],       # A persona padr√£o
        db: AsyncSession,
    ) -> Dict[str, Any]:
        """
        Usa a IA para analisar dados do sistema com base em uma pergunta do usu√°rio.
        Retorna um dicion√°rio JSON com a an√°lise estruturada.
        """
        logger.info(f"Iniciando an√°lise de dados para user_id={user.id} com a pergunta: '{question[:100]}...'")

        # 1. System Instruction
        system_instruction = (
            "Voc√™ √© um analista de dados s√™nior especialista em atendimento ao cliente.\n"
            "Sua tarefa √© analisar os dados fornecidos e responder √† pergunta do usu√°rio.\n"
            "Sua resposta DEVE ser estritamente um objeto JSON v√°lido, sem markdown de c√≥digo.\n"
            "Siga a estrutura sugerida para organizar sua an√°lise."
        )

        # 2. Processamento dos dados quantitativos (Estat√≠sticas Gerais)
        total = len(atendimentos)
        status_counts = {}
        for at in atendimentos:
            status_counts[at.status] = status_counts.get(at.status, 0) + 1
        
        stats_summary = {
            "total_atendimentos": total,
            "distribuicao_status": status_counts,
            "periodo_analisado": "Verificar datas nos filtros"
        }

        # 3. RAG em Mem√≥ria para dados qualitativos (Conversas/Observa√ß√µes)
        # Prepara textos para embedding (Limitado aos 100 mais recentes para performance)
        docs_for_embedding = []
        atendimentos_map = {} 
        
        # Ordena por data de atualiza√ß√£o (mais recentes primeiro) se ainda n√£o estiver
        sorted_atendimentos = sorted(atendimentos, key=lambda x: x.updated_at, reverse=True)[:100]

        for idx, at in enumerate(sorted_atendimentos):
            conversa_text = ""
            try:
                msgs = json.loads(at.conversa or "[]")
                # Pega as √∫ltimas 5 mensagens para contexto
                last_msgs = msgs[-5:]
                conversa_text = " | ".join([f"{m.get('role')}: {m.get('content')}" for m in last_msgs])
            except:
                conversa_text = "Sem hist√≥rico leg√≠vel."

            doc_text = (
                f"Status: {at.status}. "
                f"Observa√ß√µes: {at.observacoes or ''}. "
                f"Conversa recente: {conversa_text}"
            )
            docs_for_embedding.append(doc_text)
            atendimentos_map[idx] = at

        relevant_atendimentos_data = []
        
        if docs_for_embedding and question:
            try:
                q_embedding = await self.generate_embedding(question)
                if q_embedding:
                    doc_embeddings = await self.generate_embeddings_batch(docs_for_embedding)
                    
                    scores = []
                    q_vec = np.array(q_embedding)
                    norm_q = np.linalg.norm(q_vec)

                    for d_vec in doc_embeddings:
                        if not d_vec:
                            scores.append(-1)
                            continue
                        d_vec_np = np.array(d_vec)
                        norm_d = np.linalg.norm(d_vec_np)
                        if norm_q == 0 or norm_d == 0:
                            scores.append(0)
                        else:
                            scores.append(np.dot(q_vec, d_vec_np) / (norm_q * norm_d))
                    
                    # Seleciona Top 15 mais relevantes
                    top_indices = np.argsort(scores)[::-1][:15]
                    
                    for idx in top_indices:
                        if scores[idx] > 0.25: # Threshold de relev√¢ncia
                            at = atendimentos_map[idx]
                            relevant_atendimentos_data.append({
                                "id": at.id,
                                "nome": at.nome_contato,
                                "status": at.status,
                                "observacoes": at.observacoes,
                                "trecho_conversa": docs_for_embedding[idx]
                            })
            except Exception as e:
                logger.error(f"Erro no RAG do Dashboard: {e}")

        persona_context = None
        if persona:
            persona_context = {"nome_persona": persona.nome_config, "contexto": persona.prompt}

        analysis_prompt = {
            "pergunta_usuario": question,
            "dados_estatisticos": stats_summary,
            "dados_qualitativos_relevantes": relevant_atendimentos_data,
            "contexto_adicional": {
                "resumo_usuario": {"id": user.id, "email": user.email, "tokens_restantes": user.tokens},
                "contexto_persona_ia": persona_context or "N/A",
            },
            "instrucoes_formato": {
                "analise_de_conversao": {
                    "diagnostico_geral": "Um par√°grafo resumindo a situa√ß√£o.",
                    "principais_pontos_de_friccao": [
                        {"area": "Nome da √Årea (ex: Pre√ßos)", "observacoes": "Detalhes observados em texto simples.", "impacto_na_conversao": "Alto/M√©dio/Baixo"}
                    ],
                    "insights_acionaveis": [
                        {"titulo": "T√≠tulo da Sugest√£o", "sugestoes": ["Sugest√£o 1 em texto simples.", "Sugest√£o 2 em texto simples."]}
                    ],
                    "proximos_passos_recomendados": "Recomenda√ß√£o final."
                }
            }
        }

        prompt_str = json.dumps(analysis_prompt, ensure_ascii=False, indent=2)

        response = await self._generate_with_retry(
            prompt_str, 
            db, 
            user, 
            is_media=False, 
            system_instruction=system_instruction
        )
        analysis_json = json.loads(response.text)
        return analysis_json


_gemini_service_instance = None
def get_gemini_service():
    global _gemini_service_instance
    if _gemini_service_instance is None:
        _gemini_service_instance = GeminiService()
    return _gemini_service_instance